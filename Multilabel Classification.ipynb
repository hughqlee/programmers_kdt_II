{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X, _y = mnist[\"data\"], mnist[\"target\"]\n",
    "_y = _y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = _X[:60000], _X[60000:], _y[:60000], _y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_train_multilabel = (np.c_[y_train_large, y_train_odd]).astype(np.uint8)\n",
    "\n",
    "y_test_large = (y_test >= 7)\n",
    "y_test_odd = (y_test % 2 == 1)\n",
    "y_test_multilabel = (np.c_[y_test_large, y_test_odd]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.round(sigmoid(X @ W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    cost = - (1/N) * (np.ones((1,N)) @ (np.multiply(np.log(sigmoid(X @ W) + epsilon), T)) @ np.ones((K,1)) +\n",
    "                      np.ones((1,N)) @ (np.multiply(np.log(1 - sigmoid(X @ W) + epsilon), (1 - T))) @ np.ones((K,1)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (sigmoid(X_batch @ W) - T_batch))\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W)\n",
    "        if i % 10 == 0:\n",
    "            print(cost_history[i][0])\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 1.3862543615198037 \n",
      "\n",
      "1.3687927680890142\n",
      "1.265294299222866\n",
      "1.20327120110395\n",
      "1.1600066702280398\n",
      "1.1244887128003864\n",
      "1.0791078341301592\n",
      "1.0474679357601144\n",
      "1.023012893166785\n",
      "0.9877663661673508\n",
      "0.9714981707334052\n",
      "0.95166815510206\n",
      "0.9286685362142428\n",
      "0.9192198018158195\n",
      "0.9006387253481967\n",
      "0.8828029913434877\n",
      "0.875013497191967\n",
      "0.8653407463724194\n",
      "0.8565952275254338\n",
      "0.8287656665492664\n",
      "0.8221723403487936\n",
      "0.8115768777940852\n",
      "0.8126560989421755\n",
      "0.8013599217060996\n",
      "0.7907437340969996\n",
      "0.8086087053338149\n",
      "0.7983180152162588\n",
      "0.7771057296750794\n",
      "0.7755509315598472\n",
      "0.7543896200563878\n",
      "0.7468329521911677\n",
      "0.7405568335090087\n",
      "0.7294130686387184\n",
      "0.7289102217622789\n",
      "0.7231930680541832\n",
      "0.7137333586281058\n",
      "0.7161050317992516\n",
      "0.7124439476998532\n",
      "0.7223657872534275\n",
      "0.7254432247001\n",
      "0.7141880522748352\n",
      "0.7125708854953822\n",
      "0.702938508884878\n",
      "0.6999819136959257\n",
      "0.7043001035876821\n",
      "0.7113910036841927\n",
      "0.7067566381551484\n",
      "0.6914993047259426\n",
      "0.6797644261108942\n",
      "0.675945078018479\n",
      "0.6538186862716295\n",
      "0.6456015103992669\n",
      "0.6344649840873717\n",
      "0.6402878831028895\n",
      "0.6469224343925779\n",
      "0.6460535253482496\n",
      "0.6416074356772222\n",
      "0.6585366997523106\n",
      "0.6573350488676897\n",
      "0.660058863316016\n",
      "0.6636108651980224\n",
      "0.6687440192812825\n",
      "0.6621183947276804\n",
      "0.657127072815157\n",
      "0.6630122280825137\n",
      "0.6578123792058677\n",
      "0.6725082748996816\n",
      "0.6641151226701181\n",
      "0.6638843301894808\n",
      "0.6414878848455845\n",
      "0.6361952295738893\n",
      "0.6403803923351352\n",
      "0.6323242755250614\n",
      "0.6293617276147601\n",
      "0.6602102434594599\n",
      "0.6659883962743799\n",
      "0.665600679020571\n",
      "0.655352779530907\n",
      "0.6676514520891826\n",
      "0.6608652546196913\n",
      "0.664342367875244\n",
      "0.6547217814550753\n",
      "0.6541801956570307\n",
      "0.6717383569639428\n",
      "0.6589705279473038\n",
      "0.6631708272426575\n",
      "0.6441721011763027\n",
      "0.647687835772695\n",
      "0.6567370702022072\n",
      "0.6435470543471324\n",
      "0.6325036517387892\n",
      "0.6151914952939985\n",
      "0.609402594709465\n",
      "0.6161193415846047\n",
      "0.6140791775858869\n",
      "0.6171515400854373\n",
      "0.6106683973844407\n",
      "0.6115446161776463\n",
      "0.6404716300331699\n",
      "0.6287570464661743\n",
      "0.6317565823633389\n",
      "0.6297123853116744\n",
      "0.6249238111306172\n",
      "0.6304669133198737\n",
      "0.6057361374888128\n",
      "0.6244635646086374\n",
      "0.647651592114148\n",
      "0.6520428654328184\n",
      "0.6357955957153162\n",
      "0.645542377120327\n",
      "0.6469467646245947\n",
      "0.6651103256231301\n",
      "0.6668864005850474\n",
      "0.6548496754713957\n",
      "0.6345162011776779\n",
      "0.6288092564671492\n",
      "0.6318316356020884\n",
      "0.6420467694795774\n",
      "0.6539640138634911\n",
      "0.6455669706129021\n",
      "0.6410942456734234\n",
      "0.6681405028397878\n",
      "0.6636749423396464\n",
      "0.6493729824418064\n",
      "0.6392075977068329\n",
      "0.6369950960747381\n",
      "0.6221725512690521\n",
      "0.6239163441336996\n",
      "0.623821591483017\n",
      "0.6189401752052591\n",
      "0.6293142445787204\n",
      "0.6226620533517462\n",
      "0.6030496701632215\n",
      "0.593503296426378\n",
      "0.5758206689304552\n",
      "0.5942508545321445\n",
      "0.6112834181089548\n",
      "0.6191901894610975\n",
      "0.6145476667614989\n",
      "0.6268207180508614\n",
      "0.647485072791514\n",
      "0.651316253308114\n",
      "0.6583740373637892\n",
      "0.6512745115038108\n",
      "0.6560213489754925\n",
      "0.6515209230345727\n",
      "0.6374384077969074\n",
      "0.6262944114869838\n",
      "0.63353598553673\n",
      "0.6407938964643173\n",
      "0.6326848378189933\n",
      "0.6620851140921472\n",
      "0.6730752368495871\n",
      "0.6497366418292648\n",
      "0.6505193046683824\n",
      "0.6505498001886119\n",
      "0.6399207678521857\n",
      "0.6637189675200577\n",
      "0.6869920477842624\n",
      "0.6952772128405135\n",
      "0.6858933881946938\n",
      "0.6596215149118289\n",
      "0.645037320782035\n",
      "0.6607733513392999\n",
      "0.6486424380926721\n",
      "0.6456204956154035\n",
      "0.6415497139213271\n",
      "0.6260899154047344\n",
      "0.6276175906576866\n",
      "0.6048007611364472\n",
      "0.6097669281362308\n",
      "0.624159529686892\n",
      "0.6285799361329705\n",
      "0.6382844206539139\n",
      "0.619418988028075\n",
      "0.6049288921590508\n",
      "0.5998229735808975\n",
      "0.6152412827291085\n",
      "0.609888108559085\n",
      "0.6083000572226991\n",
      "0.6219858941275777\n",
      "0.6206763810955657\n",
      "0.6305324052704816\n",
      "0.5955694116608519\n",
      "0.5785427798475049\n",
      "0.5763095545871486\n",
      "0.5716755493265601\n",
      "0.5827213383032568\n",
      "0.5831016118655625\n",
      "0.5687206135677443\n",
      "0.553663278501207\n",
      "0.5334732607130399\n",
      "0.5445264841588322\n",
      "0.5386289817396346\n",
      "0.5587613385487504\n",
      "0.5533633037081618\n",
      "0.5401921376696311\n",
      "0.5455109325134588\n",
      "0.5594604272149384\n",
      "0.5632354551431691\n",
      "0.5691827969121733\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train_multilabel\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = sum(y_pred == y_test_multilabel)/ len(y_test_multilabel)\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
